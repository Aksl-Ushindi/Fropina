from fastapi import FastAPI, Form
from fastapi.responses import PlainTextResponse
from twilio.twiml.messaging_response import MessagingResponse
from dotenv import load_dotenv
from openai import OpenAI
import uvicorn
import os

load_dotenv()  # ğŸ” Charge les variables d'environnement du .env

app = FastAPI()

# ğŸ—ï¸ RÃ©cupÃ¨re la clÃ© API OpenAI sÃ©curisÃ©e
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ğŸ’¬ Prompt systÃ¨me de Fropina
FROPINA_PROMPT = """
Tu es Fropina, une experte en soins capillaires naturels pour cheveux afro, bouclÃ©s, crÃªpus et frisÃ©s.
Tu donnes des conseils simples, naturels, jamais de produits industriels.
Tu parles comme une grande sÅ“ur bienveillante, en langage texto.
"""

@app.post("/whatsapp", response_class=PlainTextResponse)
async def whatsapp_webhook(
    Body: str = Form(..., description="Message texte envoyÃ© par l'utilisateur"),
    From: str = Form(..., description="NumÃ©ro de l'expÃ©diteur")
):
    try:
        response = client.chat.completions.create(
            model="gpt-4",  # ğŸ” Tu peux mettre "gpt-3.5-turbo" si t'as pas accÃ¨s Ã  gpt-4
            messages=[
                {"role": "system", "content": FROPINA_PROMPT},
                {"role": "user", "content": Body}
            ]
        )
        gpt_reply = response.choices[0].message.content
    except Exception as e:
        gpt_reply = f"Oops, erreur GPT ğŸ˜¢ : {str(e)}"

    twilio_response = MessagingResponse()
    twilio_response.message(gpt_reply)
    return str(twilio_response)

# ğŸš€ Pour tester en local
if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
